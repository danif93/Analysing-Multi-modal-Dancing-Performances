{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CV_utils as cvu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition as sd\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from sklearn.feature_extraction.image import reconstruct_from_patches_2d\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_b_frames, l_bgs_frames, l_c_frames, l_gs_frames = cvu.read_frames('Video/representative/Lightness/Video attached to the ICMI 2017 Paper - Lightness.mp4')\n",
    "f_b_frames, f_bgs_frames, f_c_frames, f_gs_frames = cvu.read_frames('Video/representative/Fragility/Video attached to the ICMI 2017 Paper - Fragility.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DENSITY FRAME CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_binary_density(frames, frame_size, batch_size=10, n_overlap=0, n_skip=0):\n",
    "    \n",
    "    batch_list = [] # return value: collection of position density images grouped by batches\n",
    "    batch_density = np.zeros(frame_size) # single position density image referring to a batch of frames\n",
    "    overlap_frames = np.zeros(frame_size) # position density image of the previous batch (overlapping between batches)\n",
    "    \n",
    "    skip = 0 # consecutive frames skip counter\n",
    "    i = 0    # seen frames counter (used for the overlapping purpose)\n",
    "        \n",
    "    for frame in frames:\n",
    "        \n",
    "        # skip consecutive frames, pick one every n_skip\n",
    "        if skip != 0 and skip <= n_skip:\n",
    "            skip += 1\n",
    "            continue\n",
    "        else:\n",
    "            skip = 0\n",
    "        \n",
    "        # overlapping zone: start storing frame values in order to consider them also for the next batch\n",
    "        if i >= batch_size-n_overlap:\n",
    "            overlap_frames += frame/255\n",
    "        \n",
    "        batch_density += frame/255\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # if it has already seen batch_size imgs\n",
    "        if i%batch_size == 0:\n",
    "            \n",
    "            batch_list.append(batch_density/i)\n",
    "            batch_density = overlap_frames # start from the stored frames\n",
    "            \n",
    "            i = n_overlap\n",
    "            overlap_frames = np.zeros(frame_size)\n",
    "        \n",
    "        skip += 1\n",
    "            \n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(batch_list, label):\n",
    "    \n",
    "    for i, batch in enumerate(batch_list):\n",
    "        \n",
    "        plt.title(label+' '+str(i))\n",
    "        #plt.subplot(np.ceil(len(batch_list)/2), 2, i+1)\n",
    "        plt.subplot(len(batch_list), 1, i+1)\n",
    "        plt.imshow(batch, vmin = 0, vmax = 1)\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "batch_list = normalised_binary_density(l_b_frames, frame_size=l_b_frames[0].shape, batch_size=len(l_b_frames))\n",
    "plot_density(batch_list, 'Lightness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,80))\n",
    "batch_list = normalised_binary_density(l_b_frames, frame_size=l_b_frames[0].shape, batch_size=10, n_overlap=5, n_skip=3)\n",
    "plot_density(batch_list, 'Lightness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "batch_list = normalised_binary_density(f_b_frames, frame_size=f_b_frames[0].shape, batch_size=len(f_b_frames))\n",
    "plot_density(batch_list, 'Fragility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,80))\n",
    "batch_list = normalised_binary_density(f_b_frames, frame_size=f_b_frames[0].shape, batch_size=10, n_overlap=5, n_skip=3)\n",
    "plot_density(batch_list, 'Fragility')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DENSITY MANIPULATION and DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation_per_frame(df):\n",
    "    \n",
    "    density = []\n",
    "    \n",
    "    for r in df:\n",
    "        d = np.sum(np.square(r)) #integral over the columns of the squared function\n",
    "        if d != 0:\n",
    "            density.append(d)\n",
    "            \n",
    "    #print(len(density))\n",
    "    return sum(density)/len(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_density_feature(df_list): #for the moment I commented the columns approach\n",
    "    \n",
    "    #column_approach = []\n",
    "    row_approach = []\n",
    "\n",
    "    for df in df_list:\n",
    "    #    column_approach.append(density_estimation_per_frame(df.T))\n",
    "        row_approach.append(density_estimation_per_frame(df))\n",
    "    \n",
    "    #density_by_columns = np.sum(np.square(column_approach))/len(df_list)\n",
    "    density_by_rows =  np.sum(np.square(row_approach))/len(df_list)\n",
    "\n",
    "    \n",
    "    return density_by_rows#, density_by_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_dataset(videos_frames_list, tot_video_df_list, batch_size, n_overlap, n_skip, verbose = False):\n",
    "        \n",
    "    video_approach_density = []\n",
    "    \n",
    "    v = 0\n",
    "    tot = videos_frames_list.shape[0]\n",
    "\n",
    "    for video_frames in videos_frames_list.iterrows():\n",
    "        \n",
    "        #cleaning the None added during the dump saving\n",
    "        video_frames = video_frames[1].tolist()\n",
    "        video_frames = [x for x in video_frames if x is not None]\n",
    "        \n",
    "        tot_video_df = tot_video_df_list.iloc[v, 0]\n",
    "        \n",
    "        v += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Creating row of video {}/{}\".format(v, tot))\n",
    "        \n",
    "        density_list = []\n",
    "        #computation of the density frame by frame (global approach)\n",
    "        density_list.append(density_estimation_per_frame(tot_video_df))\n",
    "        \n",
    "        for i, n in enumerate(n_overlap):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\".... processing the approach {}/{}\".format(i+1, len(n_overlap)))\n",
    "            \n",
    "            #computation of the density at different granularity levels\n",
    "            df_list = normalised_binary_density(video_frames, frame_size=video_frames[0].shape, batch_size=batch_size[i], n_overlap=n, n_skip=n_skip[i])\n",
    "\n",
    "            #density_by_row,\n",
    "            density_by_columns = dataset_density_feature(df_list)\n",
    "            #density_list.append(density_by_row)\n",
    "            density_list.append(density_by_columns)\n",
    "\n",
    "        \n",
    "        video_approach_density.append(density_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return np.asarray(video_approach_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_density_dataset(batch_size, n_overlap, n_skip, verbose = False):\n",
    "\n",
    "    # checking input data's consistency\n",
    "    if len(n_overlap) != len(n_skip) or len(n_overlap) != len(batch_size) or len(batch_size) != len(n_skip):\n",
    "        raise Exception(\"batch_size, n_overlap and n_skip must have the same size. {}, {} and {} respectively instead\".format(len(batch_size), len(n_overlap), len(n_skip)))\n",
    "\n",
    "    j = 0\n",
    "\n",
    "    while j < 15:\n",
    "\n",
    "        j+=1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Extracting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        \n",
    "        #data extraction\n",
    "        df_video_frames = pd.read_pickle('Video/binaryFrame_pkl/videos_frames_'+str(j)+'.pkl')\n",
    "        df_density_frames = pd.read_pickle('Video/densityFrame_pkl/videos_density_'+str(j)+'.pkl')\n",
    "\n",
    "        \n",
    "        # computation of the subdataset for each subset of the videos and then their inline concatenation\n",
    "        if j == 1:\n",
    "\n",
    "            X = density_dataset(df_video_frames, df_density_frames, batch_size, n_overlap, n_skip, verbose = verbose)\n",
    "\n",
    "        else:\n",
    "\n",
    "            X = np.concatenate((X, density_dataset(df_video_frames, df_density_frames, batch_size, n_overlap, n_skip, verbose = verbose)), axis = 0)\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset's creation parameters\n",
    "batch_size = [100, 50, 50, 30, 30, 20, 20, 10, 10]\n",
    "n_overlap = [30, 10, 7, 5, 15, 5, 10, 5, 3]\n",
    "n_skip = [0, 0, 2, 3, 1, 3, 2, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call for dataset creation\n",
    "X = create_density_dataset(batch_size, n_overlap, n_skip, verbose = True)\n",
    "df = pd.DataFrame(X)\n",
    "df.to_csv(\"datasets/density_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mark loading\n",
    "\n",
    "names, Y = cvu.load_marks()\n",
    "\n",
    "Y = Y.as_matrix().astype(float)\n",
    "Y_l = Y[:,0]\n",
    "Y_f = Y[:,1]\n",
    "\n",
    "names = sorted(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"datasets/density_datasetV2.csv\").values\n",
    "\n",
    "#deleting of the entrace with issues\n",
    "delIdx = np.where(np.isnan(X))[0]\n",
    "X = np.delete(X, delIdx, axis=0)\n",
    "Y = np.delete(Y, delIdx, axis=0)\n",
    "\n",
    "Y_l = Y[:, 0]\n",
    "Y_f = Y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_l_tr, Y_f_tr, X_ts, Y_l_ts, Y_f_ts = cvu.random_sampling(X, Y_l, Y_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'alpha': list(np.arange(0.001, 1, 0.007))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LIGHTNESS\n",
    "cvu.print_results('RLS',cvu.rlsCV_regression(alphas, X_tr, X_ts, Y_l_tr, Y_l_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRAGILITY\n",
    "cvu.print_results('RLS', cvu.rlsCV_regression(alphas, X_tr, X_ts, Y_f_tr, Y_f_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'alpha': list(np.arange(0.001, 1, 0.007))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0.001, 1, 0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIGHTNESS\n",
    "#cvu.print_results('LASSO', cvu.lassoCV_regression(alphas, X_tr, X_ts, Y_l_tr, Y_l_ts))\n",
    "alpha, mean_err, var_err, coef, y_pred = cvu.lassoCV_regression(alphas, X_tr, X_ts, Y_l_tr, Y_l_ts)\n",
    "print(mean_err)\n",
    "print(var_err)\n",
    "print(coef)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRAGILITY\n",
    "#cvu.print_results('LASSO', cvu.lasosCV_regression(alphas, X_tr, X_ts, Y_f_tr, Y_f_ts))\n",
    "alpha, mean_err, var_err, coef, y_pred = cvu.lassoCV_regression(alphas, X_tr, X_ts, Y_f_tr, Y_f_ts)\n",
    "print(mean_err)\n",
    "print(var_err)\n",
    "print(coef)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {'C':list(np.arange(0.0001, 0.1, 0.01))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIGHTNESS\n",
    "cvu.print_results('SVM', cvu.svmCV_regression(c, X_tr, X_ts, Y_l_tr, Y_l_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRAGILITY\n",
    "cvu.print_results('SVM', cvu.svmCV_regression(c, X_tr, X_ts, Y_f_tr, Y_f_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'alpha': list(np.arange(0.001, 1, 0.007))}\n",
    "kernel = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIGHTNESS\n",
    "cvu.print_results('RIDGE KERNEL', cvu.ridgeKernelCV_regression(alphas, X_tr, X_ts, Y_l_tr, Y_l_ts, kernel = kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRAGILITY\n",
    "cvu.print_results('RIDGE KERNEL', cvu.ridgeKernelCV_regression(alphas, X_tr, X_ts, Y_f_tr, Y_f_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ris = cvu.tree_regression(X_tr, X_ts, Y_l_tr, Y_l_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ris = cvu.tree_regression(X_tr, X_ts, Y_f_tr, Y_f_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ris[0], ris[1])\n",
    "print(ris[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICTIONARY LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to learn the dictionary from the reference image\n",
    "def learn_reference_image(ref_image, patch_size = 7, patches = 0.02, n_atoms = 25, sparsity = 1, max_iteration = 500, verbose = False):\n",
    "    \n",
    "    if verbose:\n",
    "        start = time.time()\n",
    "        print(\"Extracting the patches\")\n",
    "        \n",
    "    \n",
    "    ref_image, _ = cvu.cut_relevant_part(ref_image)\n",
    "    D = extract_patches_2d(ref_image, patch_size, max_patches = patches)\n",
    "    D = D.reshape(D.shape[0], -1)\n",
    "\n",
    "    D -= np.mean(D, axis = 0)\n",
    "    D /= np.std(D, axis = 0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Complete. Time spent: %s\" % (time.time()-start))\n",
    "    \n",
    "    dl = sd.MiniBatchDictionaryLearning(n_components = n_atoms, alpha = sparsity, n_iter=max_iteration)\n",
    "    \n",
    "    if verbose:\n",
    "        start = time.time()\n",
    "        print(\"Learning atoms\")\n",
    "    \n",
    "    atoms = dl.fit(D).components_\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Complete. Time spent: %s\" % (time.time()-start))\n",
    "        \n",
    "        #plt.figure(figsize=(4.2, 4))\n",
    "        #for i, comp in enumerate(atoms[:100]):\n",
    "        #    plt.subplot(10, 10, i + 1)\n",
    "        #    plt.imshow(comp.reshape(patch_size),\n",
    "        #               interpolation='nearest')\n",
    "        #    plt.xticks(())\n",
    "        #    plt.yticks(())\n",
    "        #plt.suptitle('Dictionary learned from \\n' +\n",
    "        #             '%d patches' % (len(D)),\n",
    "        #             fontsize=16)\n",
    "        #plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return dl, atoms, ref_image.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean (cutting) the image to reproduce and to compute its encoding\n",
    "def reconstruct_image(image_to_analyze, dl, atoms, patch_size, width = None, non_zero_components = 6, verbose = False):\n",
    "    \n",
    "    if verbose:\n",
    "        start = time.time()\n",
    "        print(\"Extracting the patches\")\n",
    "    \n",
    "    #image cleaning from usless information\n",
    "    image_to_analyze, _ = cvu.cut_relevant_part(image_to_analyze, width)\n",
    "    I = extract_patches_2d(image_to_analyze, patch_size)\n",
    "    I = I.reshape(I.shape[0], -1) \n",
    "    intercept = np.mean(I, axis=0) \n",
    "    I -= intercept\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Complete. Time spent: %s\" % (time.time()-start))\n",
    "        start = time.time()\n",
    "        print(\"Transforming the image (could require some minutes...)\")\n",
    "\n",
    "    #reconstruction = ref_image.copy()\n",
    "    dl.set_params(transform_algorithm='omp',transform_n_nonzero_coefs=non_zero_components)\n",
    "    code = dl.transform(I)\n",
    "    reconstruction = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Complete. Time spent: %s\" % (time.time()-start))\n",
    "        #start = time.time()\n",
    "        #print(\"Reconstructing the image\")\n",
    "        \n",
    "        #patches = np.dot(code, atoms)\n",
    "\n",
    "        # recombine the image\n",
    "        #patches += intercept #denormalize\n",
    "        #patches = patches.reshape(len(I), *patch_size)\n",
    "        #reconstruction = reconstruct_from_patches_2d(patches, (image_to_analyze.shape[0], image_to_analyze.shape[1] ))\n",
    "    \n",
    "\n",
    "        #print(\"Complete. Time spent: %s\" % (time.time()-start))\n",
    "    \n",
    "    return image_to_analyze, reconstruction, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the encoding the image to reproduce and that shows the result compared with the original input\n",
    "def get_encoding(dl, atoms, image_to_analyze, patch_size,  width = None, verbose = False):\n",
    "    \n",
    "    image_to_analyze, image_reconstructed, encoding = reconstruct_image(image_to_analyze, dl, atoms, patch_size, width = width, verbose = verbose)\n",
    "    \n",
    "    #if verbose:\n",
    "    #\n",
    "    #    plt.figure(figsize=(10,10))\n",
    "    #    plt.title('Reconstructed image')\n",
    "    #    plt.imshow(image_reconstructed, vmin=0, vmax=1, interpolation='nearest')\n",
    "\n",
    "    #    plt.figure(figsize=(10,10))\n",
    "    #    plt.title('Starting image')\n",
    "    #    plt.imshow(image_to_analyze)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#child process to parallelize the density frames encoding\n",
    "def child_process(return_dict, index, dl, atoms, image_to_analyze, patch_size, width, verbose = False):\n",
    "    \n",
    "    encoding = get_encoding(dl, atoms, image_to_analyze, patch_size, width = width, verbose = verbose)\n",
    "    #reshaping used to flatten the enconding along one row of the dataset\n",
    "    encoding = encoding.reshape(-1)\n",
    "    return_dict[index] = encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the dataset matrix\n",
    "def create_dataset_from_atoms(image_to_analyze_list, dl, atoms, patch_size, width, verbose = False):\n",
    "    \n",
    "    # preparing the final matrix\n",
    "    encoding_list = np.zeros((len(image_to_analyze_list), \n",
    "                              (width-patch_size[0]+1)*(image_to_analyze_list.iloc[0,0].shape[0]-patch_size[0]+1)*(atoms.shape[0])))\n",
    "\n",
    "    p_idx = 0\n",
    "    \n",
    "    if verbose:\n",
    "        tot = image_to_analyze_list.shape[0]\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    #-----------------------------\n",
    "    manager = multiprocessing.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    #-----------------------------\n",
    "    \n",
    "    for image_to_analyze in image_to_analyze_list.iterrows():\n",
    "        \n",
    "        image_to_analyze = image_to_analyze[1][0]\n",
    "        \n",
    "        p_idx += 1\n",
    "        \n",
    "            \n",
    "        #------------------------------------------------- \n",
    "        # getting the flattened encoding of each density frame\n",
    "        process = multiprocessing.Process(target=child_process, args=[return_dict, p_idx, dl, atoms, image_to_analyze, patch_size, width, verbose])\n",
    "        jobs.append(process)\n",
    "        process.start()    \n",
    "        #-------------------------------------------------\n",
    "\n",
    "\n",
    "    for i, p in enumerate(jobs):\n",
    "        p.join()\n",
    "    \n",
    "    for i, p in enumerate(jobs):\n",
    "        encoding = return_dict[i+1]\n",
    "        #updating the matrix with a new entry\n",
    "        encoding_list[i, :] = encoding\n",
    "\n",
    "    return dl, atoms, encoding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to create subdatasets\n",
    "def create_sub_datasets(ref_image, patch_size, path_to_read, path_to_write, verbose = False):\n",
    "    \n",
    "    j = 0\n",
    "\n",
    "    print(\"Learning the reference image\\n\")\n",
    "\n",
    "    # getting the dictionary learner object, the dictionary and the width (info to clean further density frames)\n",
    "    dl, atoms, width = learn_reference_image(ref_image, patch_size, verbose = verbose)\n",
    "\n",
    "    print(\"\\nLearning complete\")\n",
    "    \n",
    "    while j < 15:\n",
    "\n",
    "        j+=1\n",
    "\n",
    "        print(\"Extracting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        df_density_frames = pd.read_pickle(path_to_read+str(j)+'.pkl')\n",
    "   \n",
    "\n",
    "        # computing the sub-datasets with batches of 10 videos \n",
    "        _, _, X = create_dataset_from_atoms(df_density_frames, dl, atoms, patch_size, width = width, verbose = verbose)\n",
    "        df = pd.DataFrame(X)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Saving the dataset for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "            start = time.time()\n",
    "\n",
    "        #saving the sub-datset\n",
    "        df.to_csv(path_to_write+str(j)+\".csv\", index=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Complete. Time spent: %s\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instruction to correctly create the dataset\n",
    "patch_size = (7,7)\n",
    "ref_image = normalised_binary_density(f_b_frames, frame_size=f_b_frames[0].shape, batch_size=len(f_b_frames))[0]\n",
    "create_sub_datasets(ref_image, patch_size, 'Video/densityFrame_pkl/videos_density_', \"datasets/dictLearn_frag/dictionary_learning_fragility_dataset_\", verbose = True)\n",
    "#cvu.merge_dataset(\"dataset/dictionary_learning_lightness_dataset\", \"dataset/dictionary_learning_lightness_dataset.csv\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPORARY DATASET REARRANGEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the (indexes of the) usless columns of a sub-datast \n",
    "def get_child(dic, index, path_to_read, range_videos, verbose = False):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Extracting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        start = time.time()\n",
    "            \n",
    "    X = pd.read_csv(path_to_read+str(j)+\".csv\").as_matrix()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Complete for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        print(\"Time spent: %s\" % (time.time()-start))\n",
    "        print(\"Getting non relevant features from the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        start = time.time()\n",
    "        \n",
    "    dic[index] = cvu.get_non_relevant_features(X)\n",
    "    \n",
    "    if verbose: \n",
    "        print(\"Complete for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        print(\"Time spent: %s\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to delete the usless columns of a sub-datast \n",
    "def cut_child(j, path_to_read, range_videos, indexes, verbose = False):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Extracting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        start = time.time()\n",
    "        \n",
    "    X = pd.read_csv(path_to_read+str(j)+\".csv\").as_matrix()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Complete for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        print(\"Time spent: %s\" % (time.time()-start))\n",
    "        print(\"Cutting non relevant features from the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        start = time.time()\n",
    "        \n",
    "        \n",
    "    cvu.cut_non_relevant_features(X, indexes)\n",
    "\n",
    "    if verbose: print(\"Rewriting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "\n",
    "    df = pd.DataFrame(X)\n",
    "    df.to_csv(path_to_read+str(j)+\".csv\", index=False)\n",
    "    \n",
    "    if verbose: \n",
    "        print(\"Complete for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        print(\"Time spent: %s\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rearrange the columns of each sub-dataset in order to group them atom by atom\n",
    "def rearrange_child(j, path_to_read, range_videos, verbose = False):\n",
    "    \n",
    "    if verbose: \n",
    "            print(\"Extracting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "            start = time.time()\n",
    "            \n",
    "    X = pd.read_csv(path_to_read+str(j)+\".csv\")\n",
    "    X = X.values()\n",
    "    \n",
    "    if verbose:\n",
    "            print(\"Complete. Time spent: %s\" % (time.time()-start))\n",
    "            print(\"Rearranging the dataset for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "            start = time.time()\n",
    "    \n",
    "    new_X = np.zeros(X.shape)\n",
    "\n",
    "    col_per_atom = X.shape[1]//25\n",
    "    # rearranging\n",
    "    for r in range(25):\n",
    "        for c in range(col_per_atom):\n",
    "            new_X[:,r*col_per_atom+c] = X[:, c*25+r]\n",
    "\n",
    "    if verbose: print(\"Rewriting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "\n",
    "    df = pd.DataFrame(new_X)\n",
    "    df.to_csv(path_to_read+str(j)+\".csv\", index=False)\n",
    "    \n",
    "    if verbose: \n",
    "        print(\"Complete for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "        print(\"Time spent: %s\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the list of the (indexes of the) usless columns of the datast \n",
    "def get_non_relevant_features(proc, path_to_read, range_videos, verbose = False):\n",
    "    \n",
    "    list_non_relevant_features = []\n",
    "    jobs = []\n",
    "    manager = multiprocessing.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    \n",
    "    for j in range_videos:\n",
    "        \n",
    "        if j != 0 and j%proc == 0:\n",
    "            \n",
    "            for p in jobs:\n",
    "                p.join()\n",
    "                \n",
    "            for i, p in enumerate(jobs):\n",
    "                list_non_relevant_features.append(return_dict[i])\n",
    "            \n",
    "            jobs = []\n",
    "            return_dict =  manager.dict()\n",
    "            \n",
    "        \n",
    "        process = multiprocessing.Process(target=get_child, args=[return_dict, j, path_to_read, range_videos, verbose])\n",
    "        jobs.append(process)\n",
    "        process.start()\n",
    "        \n",
    "        if j != range_videos[-1]:\n",
    "            for p in jobs:\n",
    "                p.join()\n",
    "                \n",
    "            for i, p in enumerate(jobs):\n",
    "                list_non_relevant_features.append(return_dict[i])\n",
    "            \n",
    "            jobs = []\n",
    "            \n",
    "    return list_non_relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to delete the usless columns of the datast \n",
    "def cut_non_relevant_features(proc, path_to_read, range_videos, indexes, verbose = False):\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    for j in range_videos:\n",
    "        \n",
    "        if j != 0 and j%proc == 0:\n",
    "            \n",
    "            for p in jobs:\n",
    "                p.join()\n",
    "            \n",
    "            jobs = []\n",
    "\n",
    "        process = multiprocessing.Process(target=cut_child, args=[j, path_to_read, range_videos, indexes, verbose])\n",
    "        jobs.append(process)\n",
    "        process.start()\n",
    "            \n",
    "        if j == range_videos[-1]:\n",
    "            \n",
    "            for p in jobs:\n",
    "                p.join()\n",
    "            \n",
    "            jobs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rearrange the columns of the dataset in order to group them atom by atom\n",
    "def rearrange_datasets(proc, path_to_read, range_videos, verbose = False):\n",
    "    jobs = []\n",
    "    \n",
    "    for j in range_videos:\n",
    "        \n",
    "        if j != 0 and j%proc == 0:\n",
    "            \n",
    "            for p in jobs:\n",
    "                p.join()\n",
    "            \n",
    "            jobs = []\n",
    "            \n",
    "        \n",
    "        process = multiprocessing.Process(target=rearrange_child, args=[j, path_to_read, range_videos, verbose])\n",
    "        jobs.append(process)\n",
    "        process.start()\n",
    "        \n",
    "        if j == range_videos[-1]:\n",
    "            \n",
    "            for p in jobs:\n",
    "                p.join()\n",
    "            \n",
    "            jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rearrange_datasets(2, \"datasets/dictLearn_frag/dictionary_learning_fragility_dataset_\", range(1,16), verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrange_datasets(2, \"datasets/dictLearn_light/dictionary_learning_lightness_dataset\", range(2,16), verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_non_relevant_features = get_non_relevant_features(2, \"datasets/dictLearn_frag/dictionary_learning_fragility_dataset_\", range(1,16), verbose = True)\n",
    "\n",
    "intersection = set(list_non_relevant_features[-1])\n",
    "\n",
    "for l in list_non_relevant_features[:-1]: intersection = set(l) & intersection\n",
    "    \n",
    "intersection = list(intersection)\n",
    "\n",
    "print(len(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_non_relevant_features(2, \"datasets/dictLearn_frag/dictionary_learning_fragility_dataset_\", range(1,16), intersection, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvu.merge_dataset(\"datasets/dictLearn_frag/dictionary_learning_fragility_dataset_\", \"datasets/dictionary_learning_fragility_dataset.csv\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_datasets_v2(path_to_read, range_videos, verbose = False):\n",
    "    \n",
    "    for j in range_videos:\n",
    "\n",
    "        X_list =[]\n",
    "        \n",
    "        if verbose: \n",
    "            print(\"Extracting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "            start = time.time()\n",
    "\n",
    "        chunksize = 1\n",
    "        for X in pd.read_csv(path_to_read+str(j)+\".csv\", chunksize=chunksize):\n",
    "            #X = chunk.values()\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Rearranging the dataset for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "                \n",
    "\n",
    "            new_X = np.zeros(X.shape)\n",
    "            print(X.shape)\n",
    "\n",
    "            col_per_atom = X.shape[1]//25\n",
    "            for r in range(25):\n",
    "                for c in range(col_per_atom):\n",
    "                    new_X[r*col_per_atom+c] = X[c*25+r]\n",
    "                    \n",
    "            X_list.append(new_X)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Complete. Time spent: %s\" % (time.time()-start))\n",
    "            print(\"Rewriting data for the videos from {} to {}\".format((j-1)*10+1, j*10))\n",
    "            start = time.time()\n",
    "\n",
    "       \n",
    "        df = pd.DataFrame(np.asmatris(X_list))\n",
    "        df.to_csv(path_to_read+str(j)+\".csv\", index=False)\n",
    "\n",
    "        if verbose: print(\"Complete. Time spent: %s\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrange_datasets_v2(\"datasets/dictLearn_light/dictionary_learning_lightness_dataset\", range(2,16), verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mark loading\n",
    "\n",
    "names, Y = cvu.load_marks()\n",
    "\n",
    "Y = Y.as_matrix().astype(float)\n",
    "Y_l = Y[:,0]\n",
    "Y_f = Y[:,1]\n",
    "\n",
    "names = sorted(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"datasets/density_datasetV2.csv\").values\n",
    "\n",
    "#deleting of the entrace with issues\n",
    "delIdx = np.where(np.isnan(X))[0]\n",
    "X = np.delete(X, delIdx, axis=0)\n",
    "Y = np.delete(Y, delIdx, axis=0)\n",
    "\n",
    "Y_l = Y[:, 0]\n",
    "Y_f = Y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_l_tr, Y_f_tr, X_ts, Y_l_ts, Y_f_ts = cvu.random_sampling(X, Y_l, Y_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'alpha': list(np.arange(0.001, 1, 0.007))}\n",
    "\n",
    "# LIGHTNESS\n",
    "cvu.print_results('RLS', cvu.rlsCV_regression(alphas, X_tr, X_ts, Y_l_tr, Y_l_ts))\n",
    "\n",
    "# FRAGILITY\n",
    "cvu.print_results('RLS', cvu.rlsCV_regression(alphas, X_tr, X_ts, Y_f_tr, Y_f_ts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
