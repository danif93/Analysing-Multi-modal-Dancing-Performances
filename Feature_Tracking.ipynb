{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import CV_utils as cvu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster as sc\n",
    "import sklearn.mixture as sm\n",
    "import multiprocessing\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_b_frames, l_bgs_frames, l_c_frames, l_gs_frames = cvu.read_frames('Video/representative/Lightness/Video attached to the ICMI 2017 Paper - Lightness.mp4')\n",
    "f_b_frames, f_bgs_frames, f_c_frames, f_gs_frames = cvu.read_frames('Video/representative/Fragility/Video attached to the ICMI 2017 Paper - Fragility.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORNERS TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = sc.KMeans(n_clusters=10)\n",
    "cl2 = sc.KMeans(n_clusters=5)\n",
    "\n",
    "for i, binary_frame in enumerate(l_b_frames):\n",
    "    gray = np.float32(binary_frame)\n",
    "    dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "    #result is dilated for marking the corners, not important\n",
    "    dst = cv2.dilate(dst,None)\n",
    "\n",
    "    # Threshold for an optimal value, it may vary depending on the image.\n",
    "    colored_frame = l_c_frames[i]\n",
    "    \n",
    "    #clustering\n",
    "    dst = np.uint8(dst)\n",
    "    dst_ind = np.argwhere(dst>0)\n",
    "    centroids = cl1.fit(dst_ind).cluster_centers_\n",
    "                    \n",
    "    centroids = cl2.fit_predict(centroids)\n",
    "\n",
    "    cvu.draw_centroids(centroids, colored_frame)\n",
    "        \n",
    "    cv2.imshow('centroids',colored_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CENTROIDS FINDING TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMEANS CLUSTERING OVER WHITE PIXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl3 = sc.KMeans(n_clusters=9)\n",
    "\n",
    "white_pixels = np.argwhere(l_b_frames[0] == 255)\n",
    "\n",
    "labels = cl3.fit_predict(white_pixels)\n",
    "\n",
    "coloured_frame = l_c_frames[0]\n",
    "\n",
    "cvu.draw_clusters(labels, white_pixels, coloured_frame)\n",
    "\n",
    "plt.imshow(coloured_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_point_mass_kmeans(binary_frame_list, colored_frame_list):\n",
    "    \n",
    "    cl3 = sc.KMeans(n_clusters=20)\n",
    "\n",
    "    for i, frame in enumerate(binary_frame_list):\n",
    "        \n",
    "        white_pixel = np.argwhere(frame == 255)\n",
    "\n",
    "        centroids = cl3.fit(white_pixel).cluster_centers_\n",
    "\n",
    "        colored_frame = cvu.draw_centroids(centroids, colored_frame_list[i])\n",
    "\n",
    "        cv2.imshow('blob',colored_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "             break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_point_mass_kmeans(f_b_frames, f_c_frames)\n",
    "key_point_mass_kmeans(l_b_frames, l_c_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMEANS CLUSTERING OVER GRAYFRAME SHAPE (less reliable than the white-pixels version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl3 = sc.KMeans(n_clusters=9)\n",
    "\n",
    "gs_shape = np.argwhere(l_bgs_frames[0] != 0)\n",
    "\n",
    "labels = cl3.fit_predict(gs_shape)\n",
    "\n",
    "coloured_frame = l_c_frames[0]\n",
    "\n",
    "cvu.draw_clusters(labels, gs_shape, coloured_frame)\n",
    "\n",
    "plt.imshow(coloured_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_point_mass_kmeans_with_gsShape(binarygray_frame_list, colored_frame_list):\n",
    "    \n",
    "    cl = sc.KMeans(n_clusters=20)\n",
    "\n",
    "    for i, bg_frame in enumerate(binarygray_frame_list):\n",
    "        \n",
    "        gs_shape = np.argwhere(bg_frame != 0)\n",
    "\n",
    "        centroids = cl.fit(gs_shape).cluster_centers_\n",
    "\n",
    "        coloured_frame = cvu.draw_centroids(centroids, colored_frame_list[i])\n",
    "\n",
    "        cv2.imshow('blob', coloured_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "             break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_point_mass_kmeans_with_gsShape(f_bgs_frames, f_c_frames)\n",
    "key_point_mass_kmeans_with_gsShape(l_bgs_frames, l_c_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARE WHITE-PIXEL WITH GRAYSCALE SHAPE K-MEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kmeans(binary_frame_list, binarygray_frame_list, colored_frame_list):\n",
    "    \n",
    "    cl_b = sc.KMeans(n_clusters=20)\n",
    "    cl_gs = sc.KMeans(n_clusters=20)\n",
    "\n",
    "    for i, (bingray_frame, binary_frame) in enumerate(zip(binarygray_frame_list, binary_frame_list)):\n",
    "        \n",
    "        white_pixel_gs = np.argwhere(bingray_frame != 0)\n",
    "        white_pixel_b = np.argwhere(binary_frame == 255)\n",
    "\n",
    "        centroids_gs = cl_gs.fit(white_pixel_gs).cluster_centers_\n",
    "        centroids_b = cl_b.fit(white_pixel_b).cluster_centers_\n",
    "        \n",
    "        colored_frame = cvu.draw_centroids(centroids_gs, colored_frame_list[i])\n",
    "        colored_frame = cvu.draw_centroids(centroids_b, colored_frame_list[i], color=[255,0,0])\n",
    "        \n",
    "        cv2.imshow('blob',colored_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "             break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_kmeans(l_b_frames, l_bgs_frames, l_c_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NESTED CLUSTERING OVER CENTROIDS (high centroids movement on adaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_clustering(binary_frames, coloured_frames):\n",
    "    cl1 = sc.KMeans(n_clusters=20)\n",
    "    cl2 = sc.KMeans(n_clusters=10)\n",
    "\n",
    "    for binary_frame, c_frame in zip(binary_frames, coloured_frames):\n",
    "        \n",
    "        white_pixel_b = np.argwhere(binary_frame == 255)\n",
    "\n",
    "        centroids_b = cl1.fit(white_pixel_b).cluster_centers_\n",
    "        \n",
    "        cvu.draw_centroids(centroids_b, c_frame, color=[255,0,0])\n",
    "\n",
    "        centroids = cl2.fit(centroids_b).cluster_centers_\n",
    "        \n",
    "        cvu.draw_centroids(centroids, c_frame)\n",
    "\n",
    "        cv2.imshow('centroids', c_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_clustering(l_b_frames, l_c_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAUSSIAN CLUSTERING OVER WHITE PIXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = sm.GaussianMixture(n_components=8)\n",
    "white_pixels = np.argwhere(l_b_frames[0] == 255)\n",
    "\n",
    "gm.fit(white_pixels)\n",
    "labels = gm.predict(white_pixels)\n",
    "\n",
    "coloured_frame = l_c_frames[0]\n",
    "\n",
    "cvu.draw_clusters(labels, white_pixels, coloured_frame)\n",
    "\n",
    "plt.imshow(coloured_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_point_mass_Gaussian(binary_frame_list, colored_frame_list):\n",
    "    \n",
    "    gm = sm.GaussianMixture(n_components=20)\n",
    "\n",
    "    for i, frame in enumerate(binary_frame_list):\n",
    "        white_pixel = np.argwhere(frame == 255)\n",
    "\n",
    "        centroids = gm.fit(white_pixel).means_\n",
    "\n",
    "        colored_frame = cvu.draw_centroids(centroids, colored_frame_list[i])\n",
    "\n",
    "        cv2.imshow('blob',colored_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "             break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_point_mass_Gaussian(l_b_frames, l_c_frames)\n",
    "# key_point_mass_Gaussian(f_b_frames, f_c_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIERARCHICAL CLUSTERING OVER WHITE PIXELS (computationally too expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = sc.AgglomerativeClustering(n_clusters=9)\n",
    "\n",
    "white_pixels = np.argwhere(l_b_frames[0] == 255)\n",
    "\n",
    "labels = hc.fit_predict(white_pixels)\n",
    "\n",
    "coloured_frame = l_c_frames[0]\n",
    "\n",
    "cvu.draw_clusters(labels, white_pixels, coloured_frame)\n",
    "\n",
    "plt.imshow(coloured_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_point_mass_Hierarchical(binary_frame_list, colored_frame_list):\n",
    "    \n",
    "    hc = sc.AgglomerativeClustering(n_clusters=9)\n",
    "\n",
    "    for i, frame in enumerate(binary_frame_list):\n",
    "        white_pixel = np.argwhere(frame == 255)\n",
    "\n",
    "        labels = hc.fit_predict(white_pixel)\n",
    "\n",
    "        centroids = []\n",
    "\n",
    "        #calculating centroids\n",
    "        for l in np.unique(labels):\n",
    "            cluster = white_pixel[labels == l]\n",
    "            centroids.append(np.mean(cluster))\n",
    "\n",
    "        colored_frame = cvu.draw_centroids(centroids, colored_frame_list[i])\n",
    "\n",
    "        cv2.imshow('blob', colored_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "             break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_point_mass_Hierarchical(l_b_frames, l_c_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPECTRAL CLUSTERING OVER WHITE PIXELS (computationally too expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = sc.SpectralClustering(n_clusters=6)\n",
    "\n",
    "white_pixels = np.argwhere(l_b_frames[0] == 255)\n",
    "\n",
    "labels = cl.fit_predict(white_pixels)\n",
    "\n",
    "coloured_frame = l_c_frames[0]\n",
    "\n",
    "cvu.draw_clusters(labels, white_pixels, coloured_frame)\n",
    "\n",
    "plt.imshow(coloured_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CENTROIDS TRACKING OVER TIME (euclidean distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_centroid(first_centroids, second_centroids, n_clusters):\n",
    "    \n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "        \n",
    "    # compute the pairwise euclidean distance between the found centroids\n",
    "    distance = euclidean_distances(first_centroids, second_centroids)\n",
    "    \n",
    "    ordered_idx = []\n",
    "    distances = []\n",
    "    \n",
    "    # order the flattened distances matrix \n",
    "    for idx_value in np.argsort(distance, axis=None):\n",
    "        # row, col will be obtained by the quotient and the remainder of the division by n_cluster\n",
    "        q, r = divmod(idx_value, n_clusters)\n",
    "        ordered_idx.append((q,r))\n",
    "        distances.append(distance[q,r])\n",
    "\n",
    "    f_idx = set(range(n_clusters))\n",
    "    s_idx = set(range(n_clusters))\n",
    "    \n",
    "    closest_idx = {}\n",
    "    \n",
    "    for row, col in ordered_idx:\n",
    "        # if empty, then already seen all centroids\n",
    "        if len(f_idx)==0: break\n",
    "        # already found\n",
    "        if (row not in f_idx) or (col not in s_idx): continue\n",
    "            \n",
    "        # old_c -> new_c, diff\n",
    "        closest_idx[str(row)] = [col, second_centroids[col]-first_centroids[row]]\n",
    "        \n",
    "        f_idx.remove(row)\n",
    "        s_idx.remove(col)\n",
    "\n",
    "    return closest_idx, np.sum(distances[:(n_clusters//2)])/(n_clusters//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_over_stdDirections(associations, n_clusters, mean_distance):\n",
    "    \n",
    "    projections = np.zeros((n_clusters, len(cvu.ref_angles)))\n",
    "    \n",
    "    for map_idx in range(n_clusters):\n",
    "        # index that follow a centroid evolution\n",
    "        next_centr = str(map_idx)\n",
    "        \n",
    "        # move along all the frames pairs (dictionaries: old->new)\n",
    "        for table in associations:\n",
    "            proj_counter = np.zeros(len(cvu.ref_angles))\n",
    "            \n",
    "            # evaluate the projection on the reference angles (defined in CV_utils.py) \n",
    "            for h in range(len(cvu.ref_angles)//2):\n",
    "\n",
    "                # project\n",
    "                proj_counter[h] = np.dot(table[next_centr][1], cvu.ref_angles[h])\n",
    "                \n",
    "                # if negative it's the origin-symmetric angle\n",
    "                if proj_counter[h] < 0:\n",
    "                    proj_counter[h+(len(cvu.ref_angles)//2)] -= proj_counter[h]\n",
    "                    proj_counter[h] = 0\n",
    "                    \n",
    "            #to zero all the projections but the maximum one -- clustering    \n",
    "            max_index = np.argmax(proj_counter)\n",
    "            proj_counter[[idx for idx in range(len(cvu.ref_angles)) if idx!=max_index]] = 0\n",
    "            # since the evolution tracking of the centroids is computed purely on geometric distances (sorted by size)\n",
    "            # some centroid will be linked to perhaps a totally different centroid (this may happen when the \n",
    "            # dancer's head disappear for instance), try to patch this scenario with a fixed distance value\n",
    "            # (possibly the distance average value of the closer centroid)\n",
    "            if proj_counter[max_index] > mean_distance: proj_counter[max_index] = mean_distance\n",
    "        \n",
    "            projections[map_idx] += np.square(proj_counter)\n",
    "            \n",
    "            next_centr = str(table[next_centr][0])\n",
    "            \n",
    "    return projections/len(associations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kMeans_clustering_over_video(return_dict, p_idx, binary_frames, n_clusters=20, verbose=False):\n",
    "    \n",
    "    cl = sc.KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        \n",
    "    associations = []\n",
    "    mean_distances = []\n",
    "        \n",
    "    former_centr = cl.fit(np.argwhere(binary_frames[0]==255)).cluster_centers_\n",
    "    skip = 1\n",
    "                \n",
    "    for f_idx, binary_frame in enumerate(binary_frames[1:]):\n",
    "        \n",
    "        if verbose: start = time.time()\n",
    "            \n",
    "        # skip consecutive frames, pick one every n_skip\n",
    "        #if skip != 0 and skip <= 3:\n",
    "         #   skip += 1\n",
    "          #  continue\n",
    "        #else:\n",
    "         #   skip = 0\n",
    "        \n",
    "        if verbose: print('\\t training on frames pair {}-{} on a total of {} frames'.format(f_idx, f_idx+1, len(binary_frames)))\n",
    "        \n",
    "        latter_centr = cl.fit(np.argwhere(binary_frames[f_idx+1]==255)).cluster_centers_\n",
    "        \n",
    "        association, mean_distance = get_related_centroid(former_centr, latter_centr, n_clusters)\n",
    "        associations.append(association)\n",
    "        mean_distances.append(mean_distance)\n",
    "\n",
    "        former_centr = latter_centr\n",
    "        \n",
    "        skip += 1\n",
    "        \n",
    "        if verbose: print('done in %s' % (time.time()-start))\n",
    "    \n",
    "    return_dict[p_idx] = cluster_over_stdDirections(associations, n_clusters, np.sum(mean_distances)/len(mean_distances))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_and_create_centroid (path_to_read_binary, path_to_write, n_centroids=20, limit_videos=0, \n",
    "                              limit_frames=4, verbose=False):\n",
    "    \n",
    "    for j in range(1,16):\n",
    "        \n",
    "        res = []\n",
    "        \n",
    "        print(\"Extracting data for the videos from {} to {}\".format(((j-1)*10)+1, j*10))\n",
    "        \n",
    "        video_frames_list = cvu.binary_read(path_to_read_binary, n=j, limit_videos=limit_videos, verbose=verbose)\n",
    "        \n",
    "        if limit_videos == 0: lv=len(video_frames_list)\n",
    "        else: lv = limit_videos\n",
    "            \n",
    "        # PARRALELISATION INITIALISATION\n",
    "        jobs = []\n",
    "        manager = multiprocessing.Manager()\n",
    "        return_dict = manager.dict()\n",
    "        \n",
    "        # ----------- PARALLEL SEGMENT\n",
    "        for i, video_frames in enumerate(video_frames_list):\n",
    "\n",
    "            if limit_frames==0: lf = len(video_frames)\n",
    "            else: lf = limit_frames\n",
    "\n",
    "            video_frames = cvu.reduce_video_len(video_frames, n_frames=lf)\n",
    "                        \n",
    "            process = multiprocessing.Process(target=kMeans_clustering_over_video, args=[return_dict, i, video_frames, n_centroids, verbose])\n",
    "            jobs.append(process)\n",
    "            process.start()\n",
    "        # ----------- END PARALLEL SEGMENT\n",
    "            \n",
    "        for i, p in enumerate(jobs):\n",
    "            p.join()\n",
    "            \n",
    "        for i, p in enumerate(jobs):\n",
    "            res.append(return_dict[i].flatten())\n",
    "        \n",
    "        pd.DataFrame(data=res).to_csv(path_to_write+'centroids_clustering_over{}_{}.csv'.format(limit_frames, j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for the videos from 1 to 10\n",
      "converting 20170113_t001_segm01.mpeg : 1 out of 10\n",
      "converting 20170113_t002_segm01.mpeg : 2 out of 10\n",
      "converting 20170113_t003_segm01.mpeg : 3 out of 10\n",
      "converting 20170113_t005_segm01.mpeg : 4 out of 10\n",
      "converting 20170113_t006_segm01.mpeg : 5 out of 10\n",
      "converting 20170113_t007_segm01.mpeg : 6 out of 10\n",
      "converting 20170113_t008_segm01.mpeg : 7 out of 10\n",
      "converting 20170113_t009_segm01.mpeg : 8 out of 10\n",
      "converting 20170113_t010_segm01.mpeg : 9 out of 10\n",
      "converting 20170113_t011_segm01.mpeg : 10 out of 10\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 8.52465271949768\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 11.00267481803894\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 6.280965805053711\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.301513910293579\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 14.75300407409668\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 14.40552806854248\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 8.972978830337524\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.922922849655151\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 12.3956139087677\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 11.631155014038086\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 16.255228757858276\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 7.572146892547607\n",
      "done in 9.18532681465149\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 10.455084085464478\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.155227899551392\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 10.342271089553833\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 11.326168060302734\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 16.82672095298767\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 16.674057006835938\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 12.803135871887207\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 10.388234853744507\n",
      "done in 9.73799204826355\n",
      "done in 12.874957799911499\n",
      "done in 9.620294094085693\n",
      "done in 17.013825178146362\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.804868936538696\n",
      "done in 9.333540201187134\n",
      "done in 12.088819980621338\n",
      "done in 10.47269082069397\n",
      "done in 7.370895147323608\n",
      "Extracting data for the videos from 11 to 20\n",
      "converting 20170113_t013_segm01.mpeg : 1 out of 10\n",
      "converting 20170113_t015_segm01.mpeg : 2 out of 10\n",
      "converting 20170113_t016_segm01.mpeg : 3 out of 10\n",
      "converting 20170113_t017_segm01.mpeg : 4 out of 10\n",
      "converting 20170113_t018_segm01.mpeg : 5 out of 10\n",
      "converting 20170113_t019_segm01.mpeg : 6 out of 10\n",
      "converting 20170113_t021_segm01.mpeg : 7 out of 10\n",
      "converting 20170113_t021_segm02.mpeg : 8 out of 10\n",
      "converting 20170113_t022_segm01.mpeg : 9 out of 10\n",
      "converting 20170113_t023_segm01.mpeg : 10 out of 10\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 8.457083940505981\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 8.258092880249023\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 10.976624011993408\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.883961200714111\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 14.284097909927368\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 16.294209957122803\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 16.93764901161194\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 8.57555603981018\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 7.642002820968628\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 13.144381046295166\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 11.45065426826477\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.726067066192627\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 6.7864439487457275\n",
      "done in 12.643742084503174\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.428408145904541\n",
      "done in 22.4946551322937\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 13.681441068649292\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 16.20308017730713\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 7.605722188949585\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.140819072723389\n",
      "done in 11.94899034500122\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 17.758454084396362\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.96733808517456\n",
      "done in 8.63093090057373\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 6.533708095550537\n",
      "done in 9.647955179214478\n",
      "done in 9.15474796295166\n",
      "done in 6.257266998291016\n",
      "done in 6.517190933227539\n",
      "done in 6.201442003250122\n",
      "Extracting data for the videos from 21 to 30\n",
      "converting 20170114_t030_segm01.mpeg : 1 out of 10\n",
      "converting 20170114_t031_segm01.mpeg : 2 out of 10\n",
      "converting 20170114_t033_segm01.mpeg : 3 out of 10\n",
      "converting 20170114_t034_segm01.mpeg : 4 out of 10\n",
      "converting 20170114_t035_segm01.mpeg : 5 out of 10\n",
      "converting 20170114_t035_segm02.mpeg : 6 out of 10\n",
      "converting 20170114_t036_segm01.mpeg : 7 out of 10\n",
      "converting 20170114_t037_segm01.mpeg : 8 out of 10\n",
      "converting 20170114_t038_segm01.mpeg : 9 out of 10\n",
      "converting 20170114_t039_segm01.mpeg : 10 out of 10\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 7.914193868637085\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 8.878267049789429\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 11.395248174667358\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 11.37378215789795\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 13.6490159034729\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 11.298666000366211\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 14.12087893486023\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 12.508628129959106\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 8.538086175918579\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 13.379868984222412\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.089112043380737\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 18.90076184272766\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 9.620707988739014\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.085167169570923\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 14.14747405052185\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 13.749013185501099\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.185946941375732\n",
      "done in 15.023051023483276\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 14.233999967575073\n",
      "done in 8.78127121925354\n",
      "done in 13.104721069335938\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 10.008743047714233\n",
      "done in 18.341919898986816\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 7.7028539180755615\n",
      "done in 15.81414794921875\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.849924802780151\n",
      "done in 8.252512693405151\n",
      "done in 9.358768939971924\n",
      "done in 6.276507139205933\n",
      "done in 5.0628180503845215\n",
      "Extracting data for the videos from 31 to 40\n",
      "converting 20170114_t040_segm01.mpeg : 1 out of 10\n",
      "converting 20170114_t041_segm01.mpeg : 2 out of 10\n",
      "converting 20170114_t042_segm01.mpeg : 3 out of 10\n",
      "converting 20170114_t042_segm02.mpeg : 4 out of 10\n",
      "converting 20170114_t043_segm01.mpeg : 5 out of 10\n",
      "converting 20170114_t044_segm01.mpeg : 6 out of 10\n",
      "converting 20170114_t047_segm01.mpeg : 7 out of 10\n",
      "converting 20170114_t049_segm01.mpeg : 8 out of 10\n",
      "converting 20170114_t049_segm02.mpeg : 9 out of 10\n",
      "converting 20170114_t049_segm03.mpeg : 10 out of 10\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 10.833827018737793\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 9.852772951126099\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.088727951049805\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.357720136642456\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 9.631188154220581\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 13.201773166656494\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 12.264477014541626\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 15.397137880325317\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 7.561909914016724\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.153939008712769\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.13367223739624\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 13.627460956573486\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.37659215927124\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 10.384354829788208\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.108788967132568\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.313701868057251\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.397382974624634\n",
      "done in 8.321200132369995\n",
      "done in 8.114017963409424\n",
      "done in 13.7865309715271\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.47766399383545\n",
      "done in 9.879958152770996\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.388010263442993\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.093547105789185\n",
      "done in 15.124810934066772\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.9443838596344\n",
      "done in 7.282341957092285\n",
      "done in 4.139668941497803\n",
      "done in 5.1536948680877686\n",
      "done in 5.688849925994873\n",
      "Extracting data for the videos from 41 to 50\n",
      "converting 20170114_t050_segm01.mpeg : 1 out of 10\n",
      "converting 20170118_t060_segm01.mpeg : 2 out of 10\n",
      "converting 20170118_t061_segm01.mpeg : 3 out of 10\n",
      "converting 20170118_t062_segm01.mpeg : 4 out of 10\n",
      "converting 20170118_t063_segm01.mpeg : 5 out of 10\n",
      "converting 20170118_t064_segm01.mpeg : 6 out of 10\n",
      "converting 20170118_t065_segm01.mpeg : 7 out of 10\n",
      "converting 20170118_t066_segm01.mpeg : 8 out of 10\n",
      "converting 20170118_t067_segm01.mpeg : 9 out of 10\n",
      "converting 20170118_t068_segm01.mpeg : 10 out of 10\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 8.359968185424805\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 9.293076038360596\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "\t training on frames pair -2-1 on a total of 4 frames\n",
      "done in 11.71227216720581\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 13.214946985244751\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.287944078445435\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 11.478893041610718\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 7.522363901138306\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.101599931716919\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.723297119140625\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 11.33202314376831\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 14.207723617553711\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 10.634318113327026\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 7.571264982223511\n",
      "done in 11.063743114471436\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.869811773300171\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 13.117419958114624\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 16.207051038742065\n",
      "\t training on frames pair -1-2 on a total of 4 frames\n",
      "done in 7.291597127914429\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 10.361968755722046\n",
      "done in 9.867980718612671\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 9.161055088043213\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 8.377705097198486\n",
      "done in 7.640771150588989\n",
      "done in 6.285804033279419\n",
      "done in 8.736805200576782\n",
      "done in 8.351447105407715\n",
      "done in 6.890707969665527\n",
      "done in 10.531423330307007\n",
      "\t training on frames pair 0-3 on a total of 4 frames\n",
      "done in 4.941522121429443\n",
      "done in 4.129554986953735\n",
      "Extracting data for the videos from 51 to 60\n",
      "converting 20170118_t070_segm01.mpeg : 1 out of 10\n",
      "converting 20170118_t070_segm02.mpeg : 2 out of 10\n",
      "converting 20170118_t080_segm01.mpeg : 3 out of 10\n",
      "converting 20170118_t080_segm02.mpeg : 4 out of 10\n",
      "converting 20170118_t081_segm01.mpeg : 5 out of 10\n",
      "converting 20170118_t084_segm01.mpeg : 6 out of 10\n",
      "converting 20170118_t084_segm02.mpeg : 7 out of 10\n",
      "converting 20170118_t086_segm01.mpeg : 8 out of 10\n",
      "converting 20170118_t087_segm01.mpeg : 9 out of 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6c6e95b96bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_and_create_centroid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Video/binary_video/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasets/centrTracking/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-010ff8e5c5a8>\u001b[0m in \u001b[0;36mload_and_create_centroid\u001b[0;34m(path_to_read_binary, path_to_write, n_centroids, limit_videos, limit_frames, verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting data for the videos from {} to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mvideo_frames_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_read_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_videos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_videos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit_videos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_frames_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/projCV/CV_utils.py\u001b[0m in \u001b[0;36mbinary_read\u001b[0;34m(path_to_read_binary, n, batch_size, limit_videos, verbose)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Capture frame-by-frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtemp_bin_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_and_create_centroid('Video/binary_video/', 'datasets/centrTracking/', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLS over CENTROID TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('datasets/centroids_clustering_over100.csv', index_col=None)#0)\n",
    "y = cvu.load_marks()[1]\n",
    "y_l = y.values[:,0]\n",
    "y_f = y.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_l_tr, y_f_tr, X_ts, y_l_ts, y_f_ts = cvu.random_sampling(X.values, y_l, y_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0.001, 1, 0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RLS\n",
      "\tregularizer alpha: 0.995\n",
      "\tmean error: 1.1063062362624956\n",
      "\tvariance of the error: 0.9561799769581893\n",
      "\tcoefficient w:\n",
      "[-0.08696062  0.27875793  0.20441986  0.24748354 -0.02472268  0.19222954\n",
      "  0.20772826  0.11535959  0.20826743  0.36613765 -0.37364226  0.14722876\n",
      "  0.19231589  0.20956967  0.05767177 -0.19297894 -0.03213832  0.19805577\n",
      "  0.08727244  0.15096401  0.02332088 -0.15220475 -0.08037439  0.01240257\n",
      "  0.08717078  0.01081448 -0.33919483  0.06825607  0.09202614  0.23307957\n",
      " -0.18613445 -0.3144973   0.11963445  0.49314889 -0.01359083  0.27156067\n",
      "  0.06804062  0.00668052  0.25078651  0.17121375 -0.06825924  0.01426705\n",
      "  0.26380184  0.01930638  0.19153358 -0.22384849  0.02495429  0.22987837\n",
      " -0.00180769 -0.24673558 -0.32443728  0.33351142 -0.32686538 -0.0119978\n",
      "  0.1198952   0.51635561 -0.03698162  0.42347381  0.01056622 -0.27369921\n",
      " -0.0843951  -0.54749212 -0.35809198  0.22126217 -0.05261709  0.18962148\n",
      " -0.29505249 -0.22175841  0.11411141 -0.17200876 -0.12560939 -0.03954967\n",
      " -0.02448974 -0.40685041 -0.10645227  0.01003285 -0.09393404 -0.38198289\n",
      " -0.01961988 -0.04358478 -0.28045948 -0.23778057 -0.44965014  0.02472609\n",
      " -0.0378035  -0.01245078  0.01447606 -0.31253651  0.09871714  0.10932562\n",
      "  0.22534853 -0.25209122 -0.0730245  -0.13648777  0.32369913 -0.00835252\n",
      "  0.18248322  0.0085031   0.24413717  0.26198746 -0.18886467 -0.25600145\n",
      "  0.41555995 -0.2134235  -0.29985292 -0.19946869  0.11881312  0.04977757\n",
      "  0.17786354  0.07077981 -0.28314656 -0.05339489 -0.03079444  0.08710129\n",
      " -0.37713241 -0.45923331 -0.35039361 -0.11477387  0.06249285 -0.05329559\n",
      "  0.20206641 -0.04615237 -0.03341329 -0.16777163  0.17636355  0.06871744\n",
      " -0.07526064 -0.23071039 -0.09004962  0.13610432 -0.26520363 -0.34158636\n",
      "  0.02386247  0.16196712  0.18268747  0.27282447 -0.13302475  0.53402154\n",
      " -0.14399464 -0.46324754  0.06918063 -0.35523835  0.09518942  0.11144695\n",
      " -0.33205217 -0.31522762  0.43679701 -0.22633573  0.11105442 -0.31292616\n",
      " -0.12911599  0.37973772 -0.07688562  0.36865622 -0.08145689  0.0641756\n",
      "  0.42602111 -0.02717403  0.45439313  0.28164819]\n",
      "\tprediction:\n",
      "[ 2.41039534  4.39317101  2.69363116  2.12474255  2.0372076  -0.60366124\n",
      "  2.50889584 -0.09957532  0.35824424 -1.57754413  2.42586417  2.18074578\n",
      "  1.58692512  1.94776598  1.90212083  1.62170788  1.03671745  2.01561538\n",
      "  0.56099654  1.1499229   2.44954408  1.67062898  4.11462002  2.60222759\n",
      " -0.7582092   3.04639696  1.71077134  2.44613205  1.50193027  1.22463555\n",
      "  1.79213203  1.82357018  0.35779621  1.68874953  2.04569886  2.03443669\n",
      "  2.02813453  1.40018604]\n"
     ]
    }
   ],
   "source": [
    "# LIGHTNESS\n",
    "cvu.print_results('RLS',cvu.rlsCV_regression(alphas, X_tr, X_ts, y_l_tr, y_l_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RLS\n",
      "\tregularizer alpha: 0.995\n",
      "\tmean error: 1.1935342976264065\n",
      "\tvariance of the error: 0.9646553161956625\n",
      "\tcoefficient w:\n",
      "[ 0.05235343 -0.16622489  0.1295035  -0.15551152 -0.14891185 -0.07815401\n",
      " -0.14684368 -0.05381465 -0.48083645 -0.17855847  0.12905513  0.40580365\n",
      " -0.01604797 -0.46068768 -0.14519515  0.18318001  0.30460946 -0.12214555\n",
      "  0.08455595 -0.30895464 -0.46150833  0.23547587  0.41925673 -0.01074456\n",
      " -0.25853983  0.06116082  0.04929415 -0.38343644 -0.0611208  -0.11226291\n",
      "  0.4396265   0.00715924  0.35903543 -0.09774982  0.08520815 -0.4007364\n",
      " -0.306144   -0.24857141 -0.07675982 -0.42090559 -0.04527686  0.55632676\n",
      " -0.36016798 -0.04336974  0.08392373  0.37529294 -0.06234452 -0.36925601\n",
      "  0.14502819 -0.18074697 -0.20245525 -0.44889021  0.8546259  -0.14882103\n",
      " -0.11412293  0.10716885  0.00840083  0.2128954  -0.03341787 -0.14221537\n",
      "  0.18609423  0.46137057  0.19556718 -0.32056341 -0.20009134  0.1098705\n",
      "  0.11040219  0.12131772  0.0479488  -0.08271644  0.00443616  0.00790424\n",
      "  0.25303151 -0.08544479  0.03057619  0.03861796 -0.0282586   0.23213891\n",
      " -0.3116117   0.36579912  0.18264165  0.24643072  0.00270106  0.14504523\n",
      " -0.1152549  -0.00484656 -0.11775551  0.38680284  0.02243423  0.00265965\n",
      "  0.06686726  0.40313087  0.03461698 -0.04924137 -0.3950032  -0.34380574\n",
      "  0.21850109 -0.0313899   0.30156153  0.0659463   0.19979653  0.39892271\n",
      " -0.6194333  -0.40183763  0.14027077 -0.01925345 -0.22376543  0.38384865\n",
      " -0.29742562  0.33638338  0.14490313  0.15733233 -0.27901703  0.1525878\n",
      "  0.02587435  0.1025906   0.22798808  0.08121744 -0.18219121  0.0512071\n",
      " -0.37691837  0.38454202 -0.06572511  0.41825493 -0.55471074 -0.08417837\n",
      "  0.06725957  0.30516735 -0.04946122  0.40945795  0.47159016  0.08449271\n",
      "  0.07045795 -0.05938316 -0.16442487 -0.70520138 -0.06201152 -0.59281286\n",
      " -0.18595448  0.34224812 -0.0614876  -0.05489823  0.71080886 -0.18182927\n",
      "  0.20105024  0.08804608 -0.70287354  0.05658362  0.06543842  0.32350389\n",
      "  0.01745101  0.38998602  0.23297744 -0.3818331   0.25567009 -0.25748731\n",
      " -0.41137212  0.28702575 -0.40402079  0.17151204]\n",
      "\tprediction:\n",
      "[-0.9808994   1.11073914  1.00700522  1.11522082  1.72984924  1.95864184\n",
      " -0.01854777  5.19410023  0.64424716  1.28784919 -0.51348331  1.68387913\n",
      "  1.86811431 -1.67227055  2.63331804  2.94351367  2.65959129  1.10227039\n",
      "  2.06644903  1.89948648  2.28702695  1.84985538  0.16754783  1.68384544\n",
      "  2.69919039 -0.3998245   1.93618188 -0.87132727  1.90867382  2.14076953\n",
      "  1.35818579  2.73848919 -3.05174726  1.68577255  3.51239596  0.47949562\n",
      "  1.97830794  4.26142515]\n"
     ]
    }
   ],
   "source": [
    "# FRAGILITY\n",
    "cvu.print_results('RLS', cvu.rlsCV_regression(alphas, X_tr, X_ts, y_f_tr, y_f_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0.001, 1, 0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LASSO\n",
      "\tregularizer alpha: 0.092\n",
      "\tmean error: 0.6589588143587017\n",
      "\tvariance of the error: 0.19131651897352492\n",
      "\tcoefficient w:\n",
      "[-0.         -0.          0.         -0.         -0.         -0.\n",
      "  0.          0.          0.          0.         -0.         -0.\n",
      "  0.          0.         -0.         -0.          0.         -0.\n",
      " -0.         -0.          0.         -0.          0.         -0.\n",
      " -0.          0.          0.          0.         -0.04659406 -0.\n",
      "  0.          0.          0.          0.         -0.         -0.\n",
      "  0.          0.          0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.          0.          0.\n",
      " -0.         -0.         -0.          0.         -0.         -0.\n",
      "  0.          0.05604347  0.          0.         -0.         -0.\n",
      " -0.         -0.          0.         -0.         -0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  0.         -0.         -0.          0.          0.         -0.07576842\n",
      "  0.          0.         -0.         -0.         -0.         -0.\n",
      "  0.          0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.          0.          0.\n",
      " -0.         -0.         -0.          0.         -0.         -0.\n",
      "  0.          0.         -0.0585641  -0.         -0.         -0.\n",
      " -0.         -0.          0.          0.          0.          0.\n",
      " -0.         -0.          0.          0.         -0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  0.         -0.         -0.          0.         -0.         -0.\n",
      " -0.          0.          0.         -0.         -0.          0.\n",
      " -0.         -0.0096197   0.         -0.          0.         -0.\n",
      " -0.         -0.         -0.          0.         -0.         -0.\n",
      "  0.          0.         -0.         -0.         -0.          0.\n",
      "  0.         -0.          0.          0.        ]\n",
      "\tprediction:\n",
      "[1.67145971 1.68025184 1.79975439 1.81405196 1.83710363 1.69845986\n",
      " 1.70298585 1.7164419  1.78291869 1.58686937 1.70802428 1.78200879\n",
      " 1.81410624 1.73163835 1.8423616  1.82970687 1.78101589 1.71075155\n",
      " 1.73185352 1.79406384 1.71167621 1.8523691  1.68417509 1.65189726\n",
      " 1.70113138 1.7363256  1.82234907 1.81185733 1.73868423 1.89331311\n",
      " 1.85741082 1.80315296 1.48599698 1.81866451 1.76561431 1.76600487\n",
      " 1.80731722 1.86391206]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#LIGHTNESS\n",
    "cvu.print_results('LASSO', cvu.lassoCV_regression(alphas, X_tr, X_ts, y_l_tr, y_l_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LASSO\n",
      "\tregularizer alpha: 0.148\n",
      "\tmean error: 0.8446709781823347\n",
      "\tvariance of the error: 0.24564075700338517\n",
      "\tcoefficient w:\n",
      "[-0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.00494228 -0.         -0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  0.         -0.         -0.         -0.          0.         -0.\n",
      " -0.11071019 -0.         -0.          0.         -0.         -0.\n",
      " -0.          0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.02324152 -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.          0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.          0.\n",
      "  0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.        ]\n",
      "\tprediction:\n",
      "[1.19966829 1.35428788 1.01885532 1.38858672 1.34217753 1.23822459\n",
      " 1.33657762 1.33977105 1.33812629 1.07243931 1.27428983 1.34298989\n",
      " 1.37739899 1.34953551 1.35167673 1.33027416 1.36943027 1.34790121\n",
      " 1.36825548 1.37914703 1.30241769 1.33825819 1.33287596 1.32380363\n",
      " 1.32377896 1.32021123 1.38463664 1.21073203 1.36515973 1.342351\n",
      " 1.38242279 1.3771276  1.28221104 1.38003072 1.32057181 1.37764929\n",
      " 1.37409545 1.22137732]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#FRAGILITY\n",
    "cvu.print_results('LASSO', cvu.lassoCV_regression(alphas, X_tr, X_ts, y_f_tr, y_f_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'alpha': np.arange(0.01, 10, 0.7),'degree':range(3,10)}\n",
    "k1 = 'poly'\n",
    "k2 = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RIDGE KERNEL\n",
      "\tregularizer alpha: 6.31\n",
      "\tmean error: 0.6788680151141095\n",
      "\tvariance of the error: 0.2438436154823583\n",
      "\tcoefficient w:\n",
      "[ 0.02828927 -0.03909785  0.03053804 -0.05836288  0.08207344  0.00610336\n",
      " -0.06243693  0.07551616 -0.10302943 -0.03180566  0.07762458 -0.01456226\n",
      "  0.04708739  0.13464499  0.21857338 -0.03580314 -0.02366532  0.00751151\n",
      " -0.02593455 -0.01422534  0.2534302   0.12324842 -0.00463337 -0.03054365\n",
      "  0.16026476  0.10970633 -0.03837178  0.08813683 -0.17394349 -0.04656924\n",
      " -0.11410814  0.05497203 -0.07943224 -0.13171103 -0.02091847 -0.18138402\n",
      " -0.03829277 -0.12276409 -0.02113168  0.11574249 -0.1319848   0.04726852\n",
      " -0.07227556  0.1175583  -0.01575829  0.28406791 -0.10952598  0.17838985\n",
      "  0.07848906 -0.00877179  0.03351943  0.27534604 -0.10189103  0.00142753\n",
      "  0.0661582  -0.03511189 -0.05725541 -0.01365932  0.22543533 -0.15434668\n",
      "  0.06993005  0.02068097  0.09230162 -0.03765378  0.1173586  -0.15520871\n",
      " -0.0316687  -0.08402437 -0.02397037  0.00586109 -0.08199195 -0.03580343\n",
      "  0.01643283  0.06916967  0.22064271 -0.06460616 -0.15393833 -0.1752423\n",
      "  0.08722619  0.27187597  0.09622896  0.01809306 -0.1233836  -0.01726068\n",
      "  0.03880941  0.09453318 -0.1137636   0.12268899  0.20493125  0.22168654\n",
      "  0.01323564 -0.16404356 -0.11683004  0.02018213 -0.08387454  0.26401039\n",
      "  0.04200795  0.01168276 -0.16861653  0.0046854  -0.10171549 -0.01422629\n",
      " -0.07751282 -0.01588286  0.31152915 -0.07677306 -0.03405638  0.11907091\n",
      "  0.11920895 -0.16493813 -0.08964913  0.16878277]\n",
      "\tprediction:\n",
      "[1.75836354 1.46834235 1.94089404 1.64653561 1.7086713  1.30126212\n",
      " 1.69515636 1.59516833 1.67368295 0.345996   1.71674188 1.66088417\n",
      " 1.51954659 1.60587147 1.7811093  1.70259396 1.64134523 1.74753811\n",
      " 1.69887212 1.58882315 1.75210966 1.74279606 1.8016009  1.5585602\n",
      " 1.59889169 1.82271354 1.5707674  1.7392527  1.57406723 1.72813175\n",
      " 1.53552054 1.79643845 0.94244643 1.57996365 1.84995937 1.68702882\n",
      " 1.73867798 2.01897632]\n"
     ]
    }
   ],
   "source": [
    "#LIGHTNESS - poly\n",
    "cvu.print_results('RIDGE KERNEL', cvu.ridgeKernelCV_regression(alphas, X_tr, X_ts, y_l_tr, y_l_ts, kernel=k1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RIDGE KERNEL\n",
      "\tregularizer alpha: 2.1099999999999994\n",
      "\tmean error: 0.6637512134697992\n",
      "\tvariance of the error: 0.22008572968188178\n",
      "\tcoefficient w:\n",
      "[ 0.07100167 -0.12120575  0.0770112  -0.12408284  0.8483982  -0.2015484\n",
      " -0.29181564  0.310217   -0.32996821 -0.14397126  0.43087289 -0.41834683\n",
      "  0.13486752  0.44334455  0.80544847 -0.12614122 -0.20874957  0.02753074\n",
      " -0.15818383 -0.38650206  0.72535714  0.43717838 -0.03015154 -0.6686621\n",
      "  0.526327    0.42239879 -0.12900433  0.27552549 -0.51519828 -0.48234669\n",
      " -0.41972959  0.2407297  -0.22531369 -0.39552768 -0.02504781 -0.74832016\n",
      " -0.15722596 -0.41669448 -0.15072743  0.78534677 -0.45473953  0.16899104\n",
      " -0.26334322  0.34900345  0.04046057  0.9131131  -0.64589075  0.75702978\n",
      "  0.26542047 -0.18960759  0.15347013  0.81038455 -0.33584148 -0.36222326\n",
      "  0.1595293  -0.11732606 -0.38988252 -0.03010262  0.63918901 -0.64941979\n",
      "  0.25105906  0.1494848   0.26922656 -0.11699443  0.42942491 -0.50503928\n",
      " -0.11303796 -0.32611895 -0.15499552 -0.00796499 -0.22035308 -0.1120089\n",
      "  0.06835702  0.23135496  0.72466468 -0.22271448 -0.48365117 -0.60350358\n",
      "  0.36798779  0.82102301  0.43244402 -0.33740641 -0.3862831  -0.02603361\n",
      "  0.18126199  0.25017273 -0.43031566  0.50895248  0.51193018  0.73366904\n",
      "  0.08875614 -0.61435969 -0.44996705  0.05483682 -0.50704251  0.82456233\n",
      "  0.0488697   0.06735958 -0.49659711  0.26954132 -0.30383196 -0.02649773\n",
      " -0.2570368   0.0486818   0.92034355 -0.22016304 -0.12935385  0.43306014\n",
      "  0.43062077 -0.49372447 -0.39376925  0.68371351]\n",
      "\tprediction:\n",
      "[1.7348841  1.82838383 1.70410207 1.68479639 1.66395215 1.81560791\n",
      " 1.73933271 1.78943583 1.75895695 1.90125015 1.80973656 1.65759114\n",
      " 1.63783335 1.71349577 1.67098327 1.66078317 1.62815894 1.67273103\n",
      " 1.68689416 1.6340239  1.75582387 1.71908712 1.69171228 1.76783659\n",
      " 1.67925413 1.67794206 1.68639634 1.65983501 1.62089975 1.64347828\n",
      " 1.73178854 1.65271071 1.82270056 1.66441295 1.70804882 1.66949739\n",
      " 1.65137696 1.84808366]\n"
     ]
    }
   ],
   "source": [
    "#LIGHTNESS - sigmoid\n",
    "cvu.print_results('RIDGE KERNEL', cvu.ridgeKernelCV_regression(alphas, X_tr, X_ts, y_l_tr, y_l_ts, kernel=k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RIDGE KERNEL\n",
      "\tregularizer alpha: 9.809999999999999\n",
      "\tmean error: 0.831779043411169\n",
      "\tvariance of the error: 0.22162139061697972\n",
      "\tcoefficient w:\n",
      "[ 0.21373178  0.15087671 -0.00156124  0.16795605 -0.03198274  0.00397148\n",
      " -0.07640199 -0.11046396  0.09713279 -0.09208949 -0.07015213  0.05631912\n",
      "  0.02811857 -0.10727042 -0.06239362  0.0425633  -0.05085845  0.03890213\n",
      " -0.0512319  -0.04721053 -0.11662321 -0.03142614  0.11939403  0.04668097\n",
      " -0.10243868 -0.07447316 -0.08024282  0.12516793  0.04295576 -0.03040095\n",
      "  0.06434405 -0.03746335  0.1429075   0.14003803  0.07052851 -0.03150858\n",
      " -0.02616056  0.0344351  -0.03534399 -0.03695053  0.10723177  0.14738807\n",
      "  0.01581576  0.19865306  0.15783622 -0.09027462  0.00823604 -0.09324578\n",
      " -0.06283044 -0.03998784  0.1398291  -0.10662383  0.17000396  0.00494747\n",
      "  0.22358106  0.06047907 -0.07394532  0.12406103 -0.07862665 -0.05152998\n",
      " -0.11585212  0.08882823  0.11189196  0.07780996  0.11082538  0.11738903\n",
      " -0.00081271 -0.04715837  0.04590593 -0.06510209  0.10149784  0.13710825\n",
      "  0.09123086  0.01796378 -0.09981602  0.03831195 -0.06728432  0.02481279\n",
      " -0.02259349 -0.08211146 -0.09368928  0.02667277 -0.00337839 -0.07116575\n",
      " -0.04877275  0.05922149  0.00576655 -0.07692067 -0.07530955 -0.13065739\n",
      " -0.00718656  0.07140246 -0.07919358  0.08699564  0.04763606 -0.08229908\n",
      "  0.22518341 -0.01178494  0.06051006  0.00367186  0.00131303 -0.11860343\n",
      "  0.04145233 -0.06962628 -0.12471559  0.03010724  0.17972526 -0.09884817\n",
      " -0.06679557 -0.0037123  -0.00201815 -0.09043079]\n",
      "\tprediction:\n",
      "[1.0089381  1.43467576 0.82393623 1.18135211 1.220187   1.47448353\n",
      " 1.54372423 1.60843881 1.20277439 0.68396014 1.3469998  1.2407611\n",
      " 1.19472228 1.36085092 1.30536898 1.21267559 1.26700842 1.33252968\n",
      " 1.61156428 1.25550925 1.17142116 1.22898821 1.31031415 1.30176641\n",
      " 1.22291331 1.29707351 1.20199431 1.19920045 1.18255812 1.2204367\n",
      " 1.1346615  1.44182392 0.89870465 1.21621555 1.30437481 1.2824005\n",
      " 1.28005226 1.84769367]\n"
     ]
    }
   ],
   "source": [
    "#FRAGILITY - poly\n",
    "cvu.print_results('RIDGE KERNEL', cvu.ridgeKernelCV_regression(alphas, X_tr, X_ts, y_f_tr, y_f_ts, kernel=k1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RIDGE KERNEL\n",
      "\tregularizer alpha: 3.51\n",
      "\tmean error: 0.861289535924411\n",
      "\tvariance of the error: 0.23803755120141415\n",
      "\tcoefficient w:\n",
      "[ 0.61485873  0.45506025 -0.00464716  0.50964858 -0.30256071 -0.27224061\n",
      " -0.23606874 -0.36141447  0.32770513 -0.35576152 -0.29111216  0.40571722\n",
      "  0.1498559  -0.28096405 -0.18047879  0.11440462 -0.23779906  0.20384434\n",
      " -0.18884316 -0.20399816 -0.35003812 -0.10253202  0.33142733  0.31767779\n",
      " -0.29105031 -0.24893051 -0.22702966  0.38700505  0.07088803 -0.20597768\n",
      "  0.16244932 -0.06797073  0.49108725  0.40570196  0.22096556 -0.08100365\n",
      " -0.07915323  0.11354237 -0.20701938 -0.28339267  0.37667912  0.45054832\n",
      "  0.09497616  0.56254487  0.49925689 -0.23030035 -0.12392053 -0.31601314\n",
      " -0.17302326 -0.25487492  0.445718   -0.29776464  0.55420053 -0.16276825\n",
      "  0.61968982  0.16815252 -0.37871729  0.38953715 -0.23851096 -0.142786\n",
      " -0.34200757  0.28172706  0.34243166  0.23311617  0.38680766  0.33484242\n",
      "  0.0551268  -0.12381039  0.08991801 -0.16568227  0.33052087  0.39572239\n",
      "  0.28022426  0.09381348 -0.23943657  0.09959939 -0.17258895  0.04483789\n",
      " -0.14633085 -0.23234224 -0.24209634  0.17952924  0.00632188 -0.17700101\n",
      " -0.36532839  0.16329465 -0.01890032 -0.25328726 -0.24924292 -0.34279412\n",
      " -0.19027621  0.27072584 -0.22569149  0.2737693   0.18949025 -0.21800236\n",
      "  0.60813782 -0.00766881  0.21059117 -0.37903779 -0.00162877 -0.33991585\n",
      "  0.16114226 -0.21918842 -0.34715085  0.11002835  0.5058715  -0.27532183\n",
      " -0.27926782  0.0038142  -0.08495393 -0.29281189]\n",
      "\tprediction:\n",
      "[1.23758168 1.39272983 1.13619603 1.25297271 1.20067955 1.36919604\n",
      " 1.28910262 1.30296695 1.34589027 1.44589045 1.31291845 1.20616641\n",
      " 1.20227512 1.30768278 1.19361322 1.16472134 1.21545689 1.21680565\n",
      " 1.3027271  1.20952707 1.25441203 1.27168526 1.2366242  1.27602586\n",
      " 1.22169435 1.23742198 1.27076207 1.18703978 1.15988782 1.17627545\n",
      " 1.29682158 1.24796309 1.35923328 1.23713498 1.2256663  1.2260619\n",
      " 1.20588307 1.39567936]\n"
     ]
    }
   ],
   "source": [
    "# FRAGILITY - sigmoid\n",
    "cvu.print_results('RIDGE KERNEL', cvu.ridgeKernelCV_regression(alphas, X_tr, X_ts, y_f_tr, y_f_ts, kernel=k2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "c={'C':np.arange(1,100,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM\n",
      "\tregularizer alpha: 1\n",
      "\tmean error: 0.7020229324548999\n",
      "\tvariance of the error: 0.2389987698782022\n",
      "\tcoefficient w:\n",
      "[[-0.71070329 -1.          1.         -0.10466542 -0.86072352  1.\n",
      "  -1.         -0.16131003  1.         -0.17378382  1.          1.\n",
      "   1.         -1.         -0.24619375  0.90311709  1.          1.\n",
      "  -1.         -1.          1.          1.         -1.          1.\n",
      "  -1.         -0.80110758 -1.          1.         -0.99421288 -1.\n",
      "  -1.         -1.         -0.06040374 -1.         -0.1139699   1.\n",
      "  -1.          1.         -0.82764693  1.          1.         -1.\n",
      "   1.          1.         -0.02303517  0.68266887  1.         -1.\n",
      "  -0.32708932  1.         -1.         -0.9110996   1.         -1.\n",
      "   1.          0.2676484   1.         -0.57486845  1.         -1.\n",
      "  -0.11849375 -1.         -1.         -1.         -1.          1.\n",
      "   1.         -1.         -1.         -1.          1.          1.\n",
      "   1.          0.08612763 -1.          1.          1.         -1.\n",
      "   1.          1.          1.          0.38199736 -1.         -1.\n",
      "   0.29525163 -1.          1.          0.97397025  0.42000567 -1.\n",
      "   0.69374702 -1.         -0.69522674  1.         -1.         -1.\n",
      "   1.          1.         -1.         -1.          1.        ]]\n",
      "\tprediction:\n",
      "[1.47075905 1.38905732 1.95742617 1.87702166 1.76071824 1.48135151\n",
      " 1.5611269  1.37206581 1.55060404 1.49896001 1.28876366 1.80298146\n",
      " 1.60723046 1.26226313 1.74491216 1.80329956 1.63389787 1.75920279\n",
      " 1.54512431 1.64582666 1.49483295 1.49159452 1.66059416 1.44819496\n",
      " 1.31733721 1.65025697 1.77245979 1.6270942  1.56657561 1.67180532\n",
      " 1.75798579 1.78122706 1.38188396 1.72289426 1.65449813 1.75922555\n",
      " 1.72598945 1.66851131]\n"
     ]
    }
   ],
   "source": [
    "#LIGHTNESS\n",
    "cvu.print_results('SVM', cvu.svmCV_regression(c, X_tr, X_ts, y_l_tr, y_l_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM\n",
      "\tregularizer alpha: 1\n",
      "\tmean error: 0.7994504484927619\n",
      "\tvariance of the error: 0.29530427284152166\n",
      "\tcoefficient w:\n",
      "[[ 1.          1.         -1.          1.         -0.28813885  0.00802393\n",
      "  -1.         -1.          1.         -1.         -1.          1.\n",
      "   0.27458427 -1.         -1.          0.75850304 -0.45496807  1.\n",
      "  -0.2371106  -0.64656478 -1.         -1.          1.          1.\n",
      "  -1.         -1.         -1.          1.          1.         -0.09466275\n",
      "   1.         -1.          1.          1.          1.         -0.36099741\n",
      "   1.         -0.65644818  1.          1.          1.          1.\n",
      "   1.         -1.          0.65973226 -1.         -1.         -0.65590699\n",
      "   1.         -1.          1.          0.40460926  1.          1.\n",
      "  -1.          1.         -1.         -0.90778065 -1.          1.\n",
      "   1.          1.          1.          1.         -1.          0.56588235\n",
      "  -1.          1.          1.          1.          0.81210002 -1.\n",
      "  -1.         -0.06301918 -0.08469478 -1.         -1.          1.\n",
      "  -0.57797132 -1.         -1.          1.         -1.         -1.\n",
      "  -1.         -1.          0.07823755  1.         -1.          1.\n",
      "   1.         -1.          1.          0.09084048 -0.19360858 -1.\n",
      "   1.         -1.         -1.          1.          1.         -1.\n",
      "  -1.         -0.52501131  0.0943703  -1.        ]]\n",
      "\tprediction:\n",
      "[0.71466375 0.77394062 0.62196989 1.31744631 1.44474275 0.72452061\n",
      " 1.42837625 1.4939817  0.66165426 0.55311871 1.12718772 1.61014362\n",
      " 1.56490133 1.08215201 1.40229512 1.40662392 1.37210003 1.44058834\n",
      " 1.60605773 1.58040931 0.91631944 0.94235951 1.21014666 0.99735434\n",
      " 0.98450626 1.15048302 1.54922132 1.13213183 1.07150367 1.32201444\n",
      " 1.39541244 1.58914981 0.42386008 1.57106571 1.1494847  1.35793894\n",
      " 1.52557817 1.04042136]\n"
     ]
    }
   ],
   "source": [
    "#FRAGILITY\n",
    "cvu.print_results('SVM', cvu.svmCV_regression(c, X_tr, X_ts, y_f_tr, y_f_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9842105263157892 0.5144875346260388\n",
      "[0.03831113 0.00034732 0.01296902 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13188249 0.0037047\n",
      " 0.         0.         0.         0.03005988 0.07227471 0.00266275\n",
      " 0.         0.00671477 0.00069463 0.         0.         0.\n",
      " 0.         0.         0.00278624 0.         0.         0.\n",
      " 0.         0.01642256 0.         0.         0.         0.00138926\n",
      " 0.00057886 0.         0.00030872 0.00368154 0.         0.\n",
      " 0.         0.         0.00037047 0.         0.00185235 0.\n",
      " 0.         0.00023154 0.         0.0008104  0.         0.\n",
      " 0.00204144 0.00038591 0.         0.01389262 0.04619297 0.\n",
      " 0.         0.         0.         0.         0.00530713 0.\n",
      " 0.         0.         0.         0.         0.         0.00216107\n",
      " 0.         0.         0.02741992 0.00208389 0.         0.\n",
      " 0.03723274 0.         0.         0.         0.00204144 0.\n",
      " 0.         0.         0.         0.01237706 0.         0.\n",
      " 0.0074094  0.         0.         0.         0.0028943  0.\n",
      " 0.         0.         0.01009531 0.         0.00192953 0.\n",
      " 0.         0.         0.09383844 0.00222282 0.00023154 0.00782618\n",
      " 0.         0.         0.0037047  0.00280168 0.         0.\n",
      " 0.         0.         0.         0.05402377 0.00852502 0.\n",
      " 0.1050468  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00092617 0.0518089  0.00030872\n",
      " 0.         0.         0.         0.         0.         0.02316339\n",
      " 0.         0.         0.         0.         0.         0.00023154\n",
      " 0.         0.00023154 0.         0.0049396  0.         0.\n",
      " 0.00046309 0.06670037 0.         0.07148762 0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "#LIGHTNESS\n",
    "ris = cvu.tree_regression(X_tr, X_ts, y_l_tr, y_l_ts)\n",
    "print(ris[0], ris[1])\n",
    "print(ris[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1631578947368424 0.700747922437673\n",
      "[0.00120971 0.         0.         0.01264331 0.         0.00095452\n",
      " 0.00187434 0.00157788 0.         0.         0.         0.00017532\n",
      " 0.         0.00023376 0.         0.         0.         0.\n",
      " 0.         0.         0.09384617 0.         0.00017532 0.00195775\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14762082 0.         0.         0.00997149 0.         0.\n",
      " 0.         0.         0.         0.00017532 0.         0.\n",
      " 0.00017532 0.         0.         0.05471461 0.00078894 0.\n",
      " 0.         0.         0.         0.         0.         0.01709792\n",
      " 0.14200974 0.02251895 0.         0.         0.         0.00093504\n",
      " 0.         0.00023376 0.         0.         0.         0.00052596\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.001461   0.00262981\n",
      " 0.         0.         0.         0.06947145 0.         0.\n",
      " 0.         0.00093504 0.00017532 0.         0.01075299 0.02222187\n",
      " 0.05425899 0.         0.00892187 0.         0.         0.\n",
      " 0.00017532 0.         0.         0.         0.         0.00017532\n",
      " 0.02545069 0.         0.01545742 0.         0.         0.00078894\n",
      " 0.         0.00157788 0.         0.         0.         0.00286357\n",
      " 0.         0.         0.         0.         0.00026298 0.\n",
      " 0.00017532 0.00035064 0.         0.         0.0540847  0.00779202\n",
      " 0.07332943 0.         0.00262981 0.         0.05697917 0.\n",
      " 0.         0.         0.         0.         0.00420769 0.\n",
      " 0.         0.04453141 0.         0.01753205 0.00657452 0.\n",
      " 0.00017532 0.00017532 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00229085 0.\n",
      " 0.         0.         0.         0.00017532]\n"
     ]
    }
   ],
   "source": [
    "#FRAGILITY\n",
    "ris = cvu.tree_regression(X_tr, X_ts, y_f_tr, y_f_ts)\n",
    "print(ris[0], ris[1])\n",
    "print(ris[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
